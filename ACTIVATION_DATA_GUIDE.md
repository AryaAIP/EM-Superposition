# Activation Data Guide

This document describes the structure and format of the residual stream activations generated by the pipeline.

## 1. Directory Structure

The data is organized hierarchically by **Model**, **Dataset**, **Split**, **Layer**, and **Shard**.

```
Datasets/Activations/
├── {model_name}/                  # e.g., unsloth_Qwen2.5-7B-Instruct
│   ├── {dataset_name}/            # e.g., SP_bad_medical_advice
│   │   ├── EM/                    # Activations for EM responses
│   │   │   ├── metadata.json      # Dataset metadata (CRITICAL)
│   │   │   ├── layer_00/          # Residual stream at Layer 0
│   │   │   │   ├── shard_000.pt   # PyTorch Tensor (Shard 0)
│   │   │   │   ├── shard_001.pt   # PyTorch Tensor (Shard 1)
│   │   │   │   └── ...
│   │   │   ├── layer_01/
│   │   │   └── ...
│   │   └── Neutral/               # Activations for Neutral responses
│   │       ├── metadata.json
│   │       ├── layer_00/
│   │       └── ...
│   └── ...
└── ...
```

## 2. File Formats

### **A. Shard Files (`shard_XXX.pt`)**
- **Format**: Standard PyTorch serialization (`torch.save`).
- **Content**: A 2D Float Tensor containing residual stream activations.
- **Shape**: `[N, hidden_dim]`
  - `N`: Number of examples in this shard (usually equal to `shard_size`, except for the last shard).
  - `hidden_dim`: The hidden dimension size of the model (e.g., 4096 for 7B models).
- **Data Type**: `float16` or `float32` (activations are moved to CPU before saving).

### **B. Metadata File (`metadata.json`)**
Located in the root of each split directory (e.g., `.../EM/metadata.json`). Contains all necessary information to reconstruct the dataset.

**Example Content:**
```json
{
  "model_name": "unsloth/Qwen2.5-7B-Instruct",
  "dataset_name": "SP_bad_medical_advice",
  "split": "EM",
  "load_in_4bit": false,
  "total_examples": 7049,
  "shard_size": 512,
  "batch_size": 32,
  "hidden_dim": 4096,
  "num_layers": 28
}
```

## 3. How to Load the Data

Here is a Python snippet to iterate over all activations for a specific model/dataset/split given the metadata.

```python
import torch
import json
from pathlib import Path

def load_activations(base_path, layer_idx):
    """
    Generator that yields batches of activations for a specific layer.
    """
    path = Path(base_path)
    layer_dir = path / f"layer_{layer_idx:02d}"
    
    # Sort shards to ensure correct order
    shard_files = sorted(layer_dir.glob("shard_*.pt"))
    
    for shard_file in shard_files:
        yield torch.load(shard_file)

# Example Usage
base_path = "Datasets/Activations/unsloth_Qwen2.5-7B-Instruct/SP_bad_medical_advice/EM"

# 1. Load Metadata
with open(f"{base_path}/metadata.json", "r") as f:
    meta = json.load(f)

print(f"Loading {meta['total_examples']} examples with dimension {meta['hidden_dim']}")

# 2. Iterate over a specific layer (e.g., Layer 15)
layer_idx = 15
all_activations = []

for shard in load_activations(base_path, layer_idx):
    all_activations.append(shard)

# 3. Combine (Optional - beware of RAM usage!)
full_tensor = torch.cat(all_activations, dim=0)
print(f"Full Layer {layer_idx} Shape: {full_tensor.shape}")
```

## 4. Notes & Caveats
- **Last Token Only**: These activations correspond **only** to the residual stream state at the **last token position** of the input sequence.
- **One Forward Pass**: Each activation comes from a single inference pass of `[Prompt] + [Response]`.
- **Sharding**: Data is sharded to allow processing datasets larger than RAM. Avoid `torch.cat` on the whole dataset unless you are sure it fits in memory.
